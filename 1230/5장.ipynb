{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwdLq-IeAwEQ"
      },
      "outputs": [],
      "source": [
        "#1501\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "y = np.arange(10) # integer label\n",
        "print(\"y=\", y)\n",
        "\n",
        "y1 = tf.keras.utils.to_categorical(y) # keras one-hot label\n",
        "print(\"y1=\", y1)\n",
        "\n",
        "##y2 = tf.one_hot(y, depth=10) # tensorflow one-hot label\n",
        "##print(\"y2=\", y2.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1502\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "CCE = tf.keras.losses.CategoricalCrossentropy()\n",
        "t= np.array([[1,   0,   0,   0],   #t[0]\n",
        "             [0,   1,   0,   0],   #t[1]\n",
        "             [0,   0,   1,   0],   #t[2]\n",
        "             [0,   0,   0,   1]])  #t[3]\n",
        " \n",
        "y =np.array([[0.4, 0.3, 0.2, 0.1], #y[0]\n",
        "             [0.1, 0.3, 0.2, 0.4]])#y[1]\n",
        "#1\n",
        "print(\"CCE(t[i], y[0])\")\n",
        "print(\"CCE(t[0], y[0])=\", CCE(t[0], y[0]).numpy() ) \n",
        "print(\"CCE(t[1], y[0])=\", CCE(t[1], y[0]).numpy() )\n",
        "print(\"CCE(t[2], y[0])=\", CCE(t[2], y[0]).numpy() )\n",
        "print(\"CCE(t[3], y[0])=\", CCE(t[3], y[0]).numpy() ) \n",
        "\n",
        "#2\n",
        "print(\"CCE(t[i], y[1])\")\n",
        "print(\"CCE(t[0], y[1])=\", CCE(t[0], y[1]).numpy() ) \n",
        "print(\"CCE(t[1], y[1])=\", CCE(t[1], y[1]).numpy() )\n",
        "print(\"CCE(t[2], y[1])=\", CCE(t[2], y[1]).numpy() )\n",
        "print(\"CCE(t[3], y[1])=\", CCE(t[3], y[1]).numpy() )\n",
        "\n",
        "#3\n",
        "print(\"CCE(np.vstack((t[1], t[1])), y)=\",\n",
        "       CCE(np.vstack((t[1], t[1])), y).numpy() )\n"
      ],
      "metadata": {
        "id": "bkImBRZKBNDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1503\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "SCE = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "t = tf.convert_to_tensor([0, 1, 2, 3])\n",
        "y =tf.convert_to_tensor([[0.4, 0.3, 0.2, 0.1], #y[0]\n",
        "                         [0.1, 0.3, 0.2, 0.4]])#y[1]\n",
        "\n",
        "#1\n",
        "print(\"SCE(t[i], y[0])\")\n",
        "print(\"SCE(t[0], y[0])=\", SCE(t[0], y[0]).numpy() ) \n",
        "print(\"SCE(t[1], y[0])=\", SCE(t[1], y[0]).numpy() )\n",
        "print(\"SCE(t[2], y[0])=\", SCE(t[2], y[0]).numpy() )\n",
        "print(\"SCE(t[3], y[0])=\", SCE(t[3], y[0]).numpy() ) \n",
        "\n",
        "#2\n",
        "print(\"SCE(t[i], y[1])\")\n",
        "print(\"SCE(t[0], y[1])=\", SCE(t[0], y[1]).numpy() ) \n",
        "print(\"SCE(t[1], y[1])=\", SCE(t[1], y[1]).numpy() )\n",
        "print(\"SCE(t[2], y[1])=\", SCE(t[2], y[1]).numpy() )\n",
        "print(\"SCE(t[3], y[1])=\", SCE(t[3], y[1]).numpy() )\n",
        "\n",
        "#3\n",
        "print(\"SCE(tf.stack((t[1], t[1])), y)=\",\n",
        "       SCE(tf.stack((t[1], t[1])), y).numpy() )\n"
      ],
      "metadata": {
        "id": "YyEvlpNjBQf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1504\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "BCE = tf.keras.losses.BinaryCrossentropy()\n",
        "t= np.array([[1,   0,   0,   0],   #t[0]\n",
        "             [0,   1,   0,   0],   #t[1]\n",
        "             [0,   0,   1,   0],   #t[2]\n",
        "             [0,   0,   0,   1]])  #t[3]\n",
        "\n",
        "y =np.array([[0.4, 0.3, 0.2, 0.1], #y[0]\n",
        "             [0.1, 0.3, 0.2, 0.4]])#y[1]\n",
        "#1\n",
        "print(\"BCE(t[i], y[0])\")\n",
        "print(\"BCE(t[0], y[0])=\", BCE(t[0], y[0]).numpy() )\n",
        "print(\"BCE(t[1], y[0])=\", BCE(t[1], y[0]).numpy() )\n",
        "print(\"BCE(t[2], y[0])=\", BCE(t[2], y[0]).numpy() )\n",
        "print(\"BCE(t[3], y[0])=\", BCE(t[3], y[0]).numpy() ) \n",
        "\n",
        "#2\n",
        "print(\"BCE(t[i], y[1])\")\n",
        "print(\"BCE(t[0], y[1])=\", BCE(t[0], y[1]).numpy() ) \n",
        "print(\"BCE(t[1], y[1])=\", BCE(t[1], y[1]).numpy() )\n",
        "print(\"BCE(t[2], y[1])=\", BCE(t[2], y[1]).numpy() )\n",
        "print(\"BCE(t[3], y[1])=\", BCE(t[3], y[1]).numpy() )\n",
        "\n",
        "#3\n",
        "print(\"BCE(np.vstack((t[0], t[0])), y)=\",\n",
        "       BCE(np.vstack((t[0], t[0])), y).numpy() )\n"
      ],
      "metadata": {
        "id": "i7DECh5EBWRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1505\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "BCE = tf.keras.losses.BinaryCrossentropy()\n",
        "t= np.array([[1,   1,   0,   0],   #t[0]\n",
        "             [0,   1,   1,   0],   #t[1]\n",
        "             [0,   0,   1,   1],   #t[2]\n",
        "             [0,   1,   0,   1]])  #t[3]\n",
        "\n",
        "y =np.array([[0.4, 0.3, 0.2, 0.1], #y[0]\n",
        "             [0.1, 0.3, 0.2, 0.4]])#y[1]\n",
        "#1\n",
        "print(\"BCE(t[i], y[0])\")\n",
        "print(\"BCE(t[0], y[0])=\", BCE(t[0], y[0]).numpy() ) \n",
        "print(\"BCE(t[1], y[0])=\", BCE(t[1], y[0]).numpy() )\n",
        "print(\"BCE(t[2], y[0])=\", BCE(t[2], y[0]).numpy() )\n",
        "print(\"BCE(t[3], y[0])=\", BCE(t[3], y[0]).numpy() ) \n",
        "\n",
        "#2\n",
        "print(\"BCE(t[i], y[1])\")\n",
        "print(\"BCE(t[0], y[1])=\", BCE(t[0], y[1]).numpy() ) \n",
        "print(\"BCE(t[1], y[1])=\", BCE(t[1], y[1]).numpy() )\n",
        "print(\"BCE(t[2], y[1])=\", BCE(t[2], y[1]).numpy() )\n",
        "print(\"BCE(t[3], y[1])=\", BCE(t[3], y[1]).numpy() )\n",
        "\n",
        "#3\n",
        "print(\"BCE(np.vstack((t[0], t[0])), y)=\",\n",
        "       BCE(np.vstack((t[0], t[0])), y).numpy() )\n"
      ],
      "metadata": {
        "id": "Dcj74EwVBblJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1601\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "x = tf.constant([-10, -1.0, 0.0, 1.0, 10], dtype = tf.float32)\n",
        "\n",
        "y1 = tf.keras.activations.linear(x) \n",
        "y2 = tf.keras.activations.sigmoid(x)\n",
        "y3 = tf.keras.activations.tanh(x)\n",
        "y4 = tf.keras.activations.relu(x)\n",
        "y5 = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "y6 = tf.keras.activations.softmax(tf.reshape(x, shape=(1, -1)))\n",
        "\n",
        "##linear = tf.keras.activations.get('linear')\n",
        "##y1 = linear(x)\n",
        "##\n",
        "##sigmoid = tf.keras.activations.get('sigmoid')\n",
        "##y2 = sigmoid(x)\n",
        "##\n",
        "##tanh = tf.keras.activations.get('tanh')\n",
        "##y3 = tanh(x)\n",
        "##\n",
        "##relu = tf.keras.activations.get('relu')\n",
        "##y4 = relu(x)\n",
        "##\n",
        "##y5 = relu(x, alpha=0.1) # LeakyReLU\n",
        "##softmax = tf.keras.activations.get('softmax')\n",
        "##y6 = softmax(tf.reshape(x, shape=(1, -1)))\n",
        "\n",
        "print(\"y1=\", y1.numpy())\n",
        "print(\"y2=\", y2.numpy())\n",
        "print(\"y3=\", y3.numpy())\n",
        "print(\"y4=\", y4.numpy())\n",
        "print(\"y5=\", y5.numpy())\n",
        "print(\"y6=\", y6.numpy())\n",
        "print(\"sum(y6)=\", np.sum(y6.numpy())) # 1.0\n"
      ],
      "metadata": {
        "id": "IfsylRDBBjIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1701\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#1\n",
        "y_true = np.array([[1, 0, 0], #0\n",
        "                   [0, 1, 0], #1\n",
        "                   [0, 0, 1], #2\n",
        "                   [1, 0, 0], #0\n",
        "                   [0, 1, 0], #1\n",
        "                   [0, 0, 1]]);#2\n",
        "\n",
        "# binary: 1 above threshold=0.5, 0 below threshold= 0.5                           \n",
        "y_pred = np.array([[0.3, 0.6, 0.1], #1\n",
        "                   [0.6, 0.3, 0.1], #0\n",
        "                   [0.1, 0.3, 0.6], #2\n",
        "                   [0.3, 0.6, 0.1], #1\n",
        "                   [0.1, 0.6, 0.3], #1\n",
        "                   [0.3, 0.1, 0.6]]);#2\n",
        "#2\n",
        "accuracy1 =tf.keras.metrics.binary_accuracy(y_true, y_pred)\n",
        "print(\"accuracy1=\", accuracy1)\n",
        "\n",
        "#2-1\n",
        "m= tf.keras.metrics.BinaryAccuracy()\n",
        "m.update_state(y_true, y_pred)\n",
        "# m.total = tf.reduce_sum(accuracy1)\n",
        "# m.count = accuracy1.shape[0]\n",
        "accuracy2 = m.result() # m.total/m.count\n",
        "print(\"m.total={}, m.count={}\".format(m.total.numpy(), m.count.numpy()))\n",
        "print(\"accuracy2=\", accuracy2.numpy())\n",
        "\n",
        "#3: calculate confusion_matrix, C\n",
        "y_true = y_true.flatten()\n",
        "y_pred = np.cast['int'](y_pred.flatten()>0.5)\n",
        "\n",
        "##y_true= tf.reshape(y_true, [y_true.shape[0]*y_true.shape[1]] )\n",
        "##y_pred= tf.cast(y_pred>0.5, y_true.dtype)\n",
        "##y_pred= tf.reshape(y_pred,  shape= y_true.shape )\n",
        "\n",
        "##y_true= tf.keras.backend.flatten(y_true)\n",
        "##y_pred= tf.cast(y_pred>0.5, tf.int32)\n",
        "##y_pred= tf.keras.backend.flatten(y_pred)\n",
        "\n",
        "print(\"y_true=\",y_true)\n",
        "print(\"y_pred=\",y_pred)\n",
        "C = tf.math.confusion_matrix(y_true, y_pred)\n",
        "print(\"confusion_matrix(C)=\", C)\n",
        "\n",
        "#4:\n",
        "m = tf.keras.metrics.Accuracy()\n",
        "m.update_state(y_true, y_pred)\n",
        "print(\"m.total={}, m.count={}\".format(m.total.numpy(), m.count.numpy()))\n",
        "accuracy3 = m.result()  # m.total/m.count\n",
        "print(\"accuracy3=\", accuracy3.numpy())\n",
        "\n",
        "#5\n",
        "#5-1\n",
        "m = tf.keras.metrics.TruePositives()\n",
        "m.update_state(y_true, y_pred)\n",
        "tp = m.result()  # m.true_positives \n",
        "print(\"tp =\", tp.numpy())\n",
        "\n",
        "#5-2\n",
        "m = tf.keras.metrics.TrueNegatives()\n",
        "m.update_state(y_true, y_pred)\n",
        "tn = m.result() # m.accumulator[0] \n",
        "print(\"tn=\", tn.numpy())\n",
        "\n",
        "#5-3\n",
        "m = tf.keras.metrics.FalsePositives()\n",
        "m.update_state(y_true, y_pred)\n",
        "fp = m.result() # m.accumulator[0] sms\n",
        "print(\"fp=\", fp.numpy())\n",
        "\n",
        "#5-4\n",
        "m = tf.keras.metrics.FalseNegatives()\n",
        "m.update_state(y_true, y_pred)\n",
        "fn = m.result()# m.accumulator[0]  \n",
        "print(\"fn=\", fn.numpy())\n",
        "\n",
        "accuracy4  = (tp + tn)/(tp+tn+fp+fn)\n",
        "precision = tp/(tp+fp)\n",
        "recall    = tp/(tp+fn)\n",
        "f1 = 2*tp/(2*tp + fp + fn) # harmonic mean of precision and recall\n",
        "print(\"accuracy4 =\", accuracy4.numpy())\n",
        "print(\"precision =\",precision.numpy())\n",
        "print(\"recall =\",   recall.numpy())\n",
        "print(\"f1 score =\", f1.numpy()) \n",
        "#6\n",
        "#6-1\n",
        "m = tf.keras.metrics.Precision()\n",
        "m.update_state(y_true, y_pred)\n",
        "print(\"m.true_positives=\", m.true_positives.numpy())\n",
        "print(\"m.false_positives\", m.false_positives.numpy())\n",
        "print(\"precision=\", m.result().numpy())\n",
        "\n",
        "#6-2\n",
        "m = tf.keras.metrics.Recall()\n",
        "m.update_state(y_true, y_pred)\n",
        "print(\"m.true_positives=\", m.true_positives.numpy())\n",
        "print(\"m.false_negatives\", m.false_negatives.numpy())\n",
        "print(\"recall=\", m.result().numpy())\n"
      ],
      "metadata": {
        "id": "UGNq0DP1Bkld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1702\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#1\n",
        "##y_true = np.array([0, 1, 2, 0, 1, 2])\n",
        "##y_true = tf.keras.utils.to_categorical(y_true) # one-hot\n",
        "y_true = np.array([[1, 0, 0], #0\n",
        "                   [0, 1, 0], #1\n",
        "                   [0, 0, 1], #2\n",
        "                   [1, 0, 0], #0\n",
        "                   [0, 1, 0], #1\n",
        "                   [0, 0, 1]]);#2\n",
        "               \n",
        "y_pred = np.array([[0.3, 0.6, 0.1],  #1,\n",
        "                   [0.6, 0.3, 0.1],  #0\n",
        "                   [0.1, 0.3, 0.6],  #2\n",
        "                   [0.3, 0.6, 0.1],  #1\n",
        "                   [0.1, 0.6, 0.3],  #1\n",
        "                   [0.3, 0.1, 0.6]]);#2\n",
        "num_class = y_true.shape[1] # 3\n",
        "\n",
        "#2: C and TOP_k\n",
        "#2-1: threshold, and C in # 3-1, #4-1, and #6 in [step1701]\n",
        "y_true1 = np.argmax(y_true, axis=1).flatten()\n",
        "y_pred1 = np.argmax(np.cast['int'](y_pred>0.5), axis=1).flatten()\n",
        "C = tf.math.confusion_matrix(y_true1, y_pred1)\n",
        "print(\"y_true1=\",y_true1) # y_true1= [0 1 2 0 1 2]\n",
        "print(\"y_pred1=\",y_pred1) # y_pred1= [1 0 2 1 1 2]\n",
        "print(\"confusion_matrix(C)=\", C)\n",
        "\n",
        "#2-2: to find top-k index, in #3-2, #4-2\n",
        "k=2\n",
        "indx = tf.argsort(y_pred, axis=1, direction='DESCENDING')\n",
        "TOP_k = indx[:,:k]\n",
        "print(\"TOP_k = \", TOP_k)\n",
        "\n",
        "#3\n",
        "print(\"In each class, precision!\")\n",
        "#3-1: binary(1 above threshold=0.5, 0 below threshold= 0.5)  \n",
        "for i in range(num_class):\n",
        "    m = tf.keras.metrics.Precision(class_id = i)\n",
        "    m.update_state(y_true, y_pred)\n",
        "    tp = m.true_positives.numpy()\n",
        "    fp = m.false_positives.numpy()\n",
        "    p = m.result().numpy()\n",
        "    print(\" p_{} ={}, tp={}, fp= {}\".format(i,p, tp, fp))\n",
        "    \n",
        "#3-2: the top-k classes with the highest predicted values\n",
        "print(\"In each class, precision with top_k=\", k)\n",
        "for i in range(num_class):\n",
        "    m = tf.keras.metrics.Precision(top_k=k, class_id = i)\n",
        "    m.update_state(y_true, y_pred)\n",
        "    tp = m.true_positives.numpy()\n",
        "    fp = m.false_positives.numpy()\n",
        "    p = m.result().numpy()\n",
        "    print(\" p_{} ={}, tp={}, fp= {}\".format(i,p, tp, fp))\n",
        "#4 \n",
        "print(\"In each class, recall!\")\n",
        "#4-1: binary(1 above threshold=0.5, 0 below threshold= 0.5)\n",
        "for i in range(num_class):\n",
        "    m = tf.keras.metrics.Recall(class_id = i)\n",
        "    m.update_state(y_true, y_pred)\n",
        "    tp = m.true_positives.numpy()\n",
        "    fn = m.false_negatives.numpy()\n",
        "    r = m.result().numpy()\n",
        "    print(\" recall_{} ={}, tp={}, fn= {}\".format(i,r, tp, fn))\n",
        "\n",
        "#4-2: the top-k classes with the highest predicted values\n",
        "print(\"In each class, recall with top_k=\", k)\n",
        "for i in range(num_class):\n",
        "    m = tf.keras.metrics.Recall(top_k=k, class_id = i)\n",
        "    m.update_state(y_true, y_pred)\n",
        "    r = m.result().numpy()\n",
        "    print(\" recall_{} ={}, tp={}, fn= {}\".format(i,r, tp, fn))\n"
      ],
      "metadata": {
        "id": "c-jhlbfnBnrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1703\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#1\n",
        "y_true = np.array([0, 1, 2, 0, 1, 2])\n",
        "y_pred = np.array([1, 0, 2, 1, 1, 2])\n",
        "\n",
        "#2 \n",
        "m = tf.keras.metrics.Accuracy()\n",
        "m.update_state(y_true, y_pred)  #m.count = 3, m.total=6\n",
        "print(\"accuracy from f.keras.metrics.Accuracy()=\", m.result().numpy() )\n",
        "\n",
        "#3\n",
        "C = tf.math.confusion_matrix(y_true, y_pred)\n",
        "print(\"confusion_matrix=\", C)\n",
        "\n",
        "correct = tf.linalg.diag_part(C)\n",
        "col_sum = tf.reduce_sum(C, axis=0)\n",
        "row_sum = tf.reduce_sum(C, axis=1)\n",
        "total   = tf.reduce_sum(C)  #  len(y_true), len(y_pred)\n",
        "\n",
        "accuracy    = tf.reduce_sum(correct)/total\n",
        "precision_i = correct/col_sum\n",
        "recall_i    = correct/row_sum\n",
        "f1_i = 2*(precision_i*recall_i)/(precision_i+recall_i) # harmonic mean of precision and recall\n",
        "f1_i = tf.where(tf.math.is_nan(f1_i), tf.zeros_like(f1_i), f1_i) # nan to 0.0\n",
        "print(\"accuracy=\",    accuracy.numpy())\n",
        "print(\"precision_i=\", precision_i.numpy())\n",
        "print(\"recall_i=\",    recall_i.numpy())\n",
        "print(\"f1_i=\",    f1_i.numpy())\n",
        "\n",
        "#4:  micro, macro, weighted avg in precision, recall \n",
        "tp = tf.reduce_sum(correct)    # notice : correct pairs such as (0,0), (1,1), (2,2)\n",
        "fp = tf.reduce_sum(col_sum - correct) # in this case, fp == fn\n",
        "fn = tf.reduce_sum(row_sum - correct) \n",
        "precision = tp/(tp + fp)  \n",
        "recall    = tp/(tp + fn)\n",
        "\n",
        "count = tf.math.bincount(y_true) # support  in sklearn.metrics\n",
        "print(\"count =\", count)\n",
        "print(\"precision(micro avg)=\", precision.numpy())\n",
        "print(\"precision(macro avg)=\", tf.reduce_sum(precision_i)/precision_i.shape[0])\n",
        "w=  tf.cast(count, dtype = tf.float64)/y_true.shape[0]  # tf.cast(total, dtype = tf.float64)\n",
        "weightedAvgP = tf.reduce_sum(precision_i*w)\n",
        "print(\"precision(weighted avg)=\", weightedAvgP)\n",
        "\n",
        "print(\"recall(micro avg)=\", recall.numpy())\n",
        "print(\"recall(macro avg)=\", tf.reduce_sum(recall_i)/recall_i.shape[0])\n",
        "weightedAvgR = tf.reduce_sum(recall_i*w)\n",
        "print(\"recall(weighted avg)=\", weightedAvgR)\n",
        "\n",
        "#5: pip install sklearn\n",
        "from sklearn.metrics import confusion_matrix, classification_report \n",
        "from sklearn.metrics import accuracy_score,precision_score, recall_score\n",
        "print(\"--- sklearn.metrics ---\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "print(\"accuracy=\", accuracy_score(y_true, y_pred)) # normalize=True\n",
        "print(\"precision_i=\", precision_score(y_true, y_pred, average=None))\n",
        "print(\"precision(micro avg)=\", precision_score(y_true, y_pred, average='micro'))\n",
        "print(\"precision(macro avg)=\", precision_score(y_true, y_pred, average='macro'))\n",
        "\n",
        "print(\"recall_i=\", recall_score(y_true, y_pred, average=None))\n",
        "print(\"recall(micro avg)=\", recall_score(y_true, y_pred, average='micro'))\n",
        "print(\"recall(macro avg)=\", recall_score(y_true, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "id": "SuNx3jmFBrdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1704\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#1\n",
        "##y_true = np.array([0, 1, 2, 0, 1, 2])\n",
        "##y_true = tf.keras.utils.to_categorical(y_true) # one-hot\n",
        "y_true = np.array([[1, 0, 0], #0\n",
        "                   [0, 1, 0], #1\n",
        "                   [0, 0, 1], #2\n",
        "                   [1, 0, 0], #0\n",
        "                   [0, 1, 0], #1\n",
        "                   [0, 0, 1]]);#2\n",
        "          \n",
        "y_pred = np.array([[0.3, 0.6, 0.1], #1\n",
        "                   [0.6, 0.3, 0.1], #0\n",
        "                   [0.1, 0.3, 0.6], #2\n",
        "                   [0.3, 0.6, 0.1], #1\n",
        "                   [0.1, 0.6, 0.3], #1\n",
        "                   [0.3, 0.1, 0.6]]);#2\n",
        "\n",
        "#2: using one-hot encoding in y_true\n",
        "print(\"CategoricalAccuracy!\")\n",
        "\n",
        "#2-1\n",
        "accuracy2_1= tf.keras.metrics.categorical_accuracy(y_true, y_pred)\n",
        "print(\"accuracy2_1=\", accuracy2_1.numpy())\n",
        "#2-2\n",
        "m = tf.keras.metrics.CategoricalAccuracy()\n",
        "m.update_state(y_true, y_pred)\n",
        "# m.total = tf.reduce_sum(accuracy2_1)\n",
        "# m.count = accuracy2_1.shape[0]\n",
        "accuracy2_2 = m.result() # m.total/m.count\n",
        "print(\"m.total={}, m.count={}\".format(m.total.numpy(),m.count.numpy()))\n",
        "print(\"accuracy2_2=\", accuracy2_2.numpy())\n",
        "\n",
        "#2-3\n",
        "top_k = 2 \n",
        "accuracy2_3 = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=top_k) \n",
        "print(\"top_k={}, accuracy2_3={}\".format(top_k, accuracy2_3))\n",
        "\n",
        "#2-4\n",
        "m = tf.keras.metrics.TopKCategoricalAccuracy(k=top_k) # default k = 5\n",
        "m.update_state(y_true, y_pred)\n",
        "# m.total = tf.reduce_sum(accuracy2_3)\n",
        "# m.count = accuracy2_3.shape[0]\n",
        "accuracy2_4 = m.result()\n",
        "print(\"m.total={}, m.count={}\".format(m.total.numpy(),m.count.numpy()))\n",
        "print(\"top_k={}, accuracy2_4={}\".format(top_k, accuracy2_4.numpy()))\n",
        "\n",
        "#3: using integer label in y_true\n",
        "print(\"SparseCategoricalAccuracy!\")\n",
        "y_true = tf.argmax(y_true, axis = 1) # np.argmax(y_true, axis = 1)\n",
        "y_true = tf.reshape(y_true, (-1,1))  # np.reshape(y_true, (-1, 1))\n",
        "print(\"y_true=\", y_true)\n",
        "\n",
        "#3-1\n",
        "accuracy3_1= tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "print(\"accuracy3_1=\", accuracy3_1.numpy())\n",
        "#3-2\n",
        "m = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "m.update_state(y_true, y_pred)\n",
        "# m.total = tf.reduce_sum(accuracy3_1)\n",
        "# m.count = accuracy3_1.shape[0]\n",
        "accuracy3_2 = m.result() # m.total/m.count\n",
        "print(\"m.total={}, m.count={}\".format(m.total.numpy(),m.count.numpy()))\n",
        "print(\"accuracy3_2=\", accuracy3_2.numpy())\n",
        "\n",
        "#3-3\n",
        "top_k = 2 \n",
        "accuracy3_3 = tf.keras.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=top_k) \n",
        "print(\"top_k={}, accuracy3_3={}\".format(top_k, accuracy3_3))\n",
        "\n",
        "#3-4\n",
        "m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=top_k) # default k = 5\n",
        "m.update_state(y_true, y_pred)\n",
        "# m.total = tf.reduce_sum(accuracy3_3)\n",
        "# m.count = accuracy3_3.shape[0]\n",
        "accuracy3_4 = m.result()\n",
        "print(\"m.total={}, m.count={}\".format(m.total.numpy(),m.count.numpy()))\n",
        "print(\"top_k={}, accuracy3_4={}\".format(top_k, accuracy3_4.numpy()))\n"
      ],
      "metadata": {
        "id": "C6_byUDFBux9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1801\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]], dtype = np.float32)\n",
        "y_true = np.array([[0],[0], [0],[1]], dtype = np.float32)   # AND\n",
        "##y_true = np.array([[0],[1],[1],[1]], dtype = np.float32)  # OR\n",
        "\n",
        "#2\n",
        "model = tf.keras.Sequential()\n",
        "# activation=tf.keras.activations.sigmoid\n",
        "model.add(tf.keras.layers.Dense(units=1, input_dim=2, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "#3\n",
        "# model.optimizer.lr: 0.001\n",
        "##model.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy']) \n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n",
        "##model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "ret = model.fit(X, y_true, epochs=100, batch_size=4, verbose=0) #silent\n",
        "##print(\"len(model.layers):\", len(model.layers)) # 1\n",
        "##loss = ret.history['loss']\n",
        "##plt.plot(loss)\n",
        "##plt.xlabel('epochs')\n",
        "##plt.ylabel('loss')\n",
        "##plt.show()\n",
        "\n",
        "#4\n",
        "test_loss, test_acc = model.evaluate(X, y_true, verbose=2)\n",
        "print(\"test_loss:\", test_loss)\n",
        "print(\"test_acc:\", test_acc)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "print(\"y_pred:\", y_pred)\n",
        "\n",
        "y_label = (y_pred> 0.5).astype(int) # Z = np.round(Z)\n",
        "print(\"y_label:\", y_label)\n",
        "\n",
        "#5: calculate the decision boundary line, w0*x + w1*y + b = 0\n",
        "##print(model.get_weights())\n",
        "w0, w1 = model.layers[0].weights[0].numpy().flatten()\n",
        "b = model.layers[0].bias.numpy()[0]\n",
        "print(\"{:>.2f}*x {:+.2f}*y {:+.2f} = 0\".format(w0, w1, b))\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "label = y_true.flatten()\n",
        "##plt.scatter(X[:, 0], X[:,1], c=label, s = 100)\n",
        "plt.scatter(X[label==0, 0], X[label==0, 1], marker='x', c=\"blue\", s= 100)\n",
        "plt.scatter(X[label==1, 0], X[label==1, 1], marker='o', c=\"green\",s= 100)\n",
        "##for x,target in zip(X, y_true):\n",
        "##        plt.plot(x[0],x[1],'go' if (target == 1.0) else 'bx', markersize= 10)\n",
        "\n",
        "def clippingLineBox(line, box):\n",
        "    w0, w1, b = line\n",
        "    xmin, xmax, ymin, ymax = box\n",
        "    \n",
        "    y0 =-(w0*xmin + b)/w1\n",
        "    y1 =-(w0*xmax + b)/w1\n",
        "\n",
        "    x0 = -(w1*ymin + b)/w0\n",
        "    x1 = -(w1*ymax + b)/w0    \n",
        "\n",
        "    xpoints = []\n",
        "    ypoints = []\n",
        "    if ymin <= y0 <= ymax:\n",
        "        xpoints.append(xmin)\n",
        "        ypoints.append(y0)\n",
        "    if ymin <= y1 <= ymax:\n",
        "        xpoints.append(xmax)\n",
        "        ypoints.append(y1)\n",
        "        \n",
        "    if xmin <= x0 <= xmax:\n",
        "        xpoints.append(x0)\n",
        "        ypoints.append(ymin)\n",
        "    if xmin <= x1 <= xmax:\n",
        "        xpoints.append(x1)\n",
        "        ypoints.append(ymax)\n",
        "    return xpoints, ypoints\n",
        "           \n",
        "# clip the line against a box, and draw\n",
        "xpoints, ypoints = clippingLineBox(line=(w0, w1, b), box=(0, 1, 0, 1))\n",
        "plt.plot(xpoints, ypoints, color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k4eWvgjaBx-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1802\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]], dtype = np.float32)\n",
        "\n",
        "y_true = np.array([[0],[0], [0],[1]], dtype = np.float32)    # AND\n",
        "##y_true = np.array([[0],[1], [1],[1]], dtype = np.float32)  # OR\n",
        "\n",
        "#2\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=1, input_dim=2, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "##model = tf.keras.Sequential()\n",
        "##model.add(tf.keras.layers.Input(shape = (2,))) # shape = 2\n",
        "##model.add(tf.keras.layers.Dense(units=1))\n",
        "##model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "##model.summary()\n",
        "\n",
        "#3\n",
        "##opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "ret = model.fit(X, y_true, epochs=100, batch_size=4, verbose=0)\n",
        "##print(\"len(model.layers):\", len(model.layers)) # 1\n",
        "##loss = ret.history['loss']\n",
        "##plt.plot(loss)\n",
        "##plt.xlabel('epochs')\n",
        "##plt.ylabel('loss')\n",
        "##plt.show()\n",
        "\n",
        "#4\n",
        "test_loss, test_acc = model.evaluate(X, y_true, verbose=2)\n",
        "print(\"test_loss:\", test_loss)\n",
        "print(\"test_acc:\", test_acc)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "print(\"y_pred:\", y_pred)\n",
        "\n",
        "y_label = (y_pred> 0.5).astype(int)\n",
        "print(\"y_label:\", y_label)\n",
        "\n",
        "#5: calculate the decision boundary line, w0*x + w1*y + b = 0\n",
        "##print(model.get_weights())\n",
        "w0, w1 = model.layers[0].weights[0].numpy().flatten()\n",
        "b = model.layers[0].bias.numpy()[0]\n",
        "print(\"{:>.2f}*x {:+.2f}*y {:+.2f} = 0\".format(w0, w1, b))\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "label = y_true.flatten()\n",
        "##plt.scatter(X[:, 0], X[:,1], c=label, s = 100)\n",
        "plt.scatter(X[label==0, 0], X[label==0,1], marker='x', s = 100)\n",
        "plt.scatter(X[label==1, 0], X[label==1,1], marker='o', s = 100)\n",
        "\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min()-h, X[:, 0].max()+h\n",
        "y_min, y_max = X[:, 1].min()-h, X[:, 1].max()+h\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "sample = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model.predict(sample) \n",
        "Z = (Z> 0.5).astype(int) # Z = np.round(Z)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contour(xx, yy, Z, colors='red', linewidths=2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lTjFqT37B2yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1901\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]], dtype = np.float32)\n",
        "\n",
        "y_true = np.array([[0],[0], [0],[1]], dtype = np.float32)   # AND\n",
        "##y_true = np.array([[0],[1],[1],[1]], dtype = np.float32)  # OR\n",
        "\n",
        "#2\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=2, input_dim=2, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "##opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "model.compile(optimizer=opt,\n",
        "               loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "ret = model.fit(X, y_true, epochs=100, batch_size=4, verbose=0)\n",
        "##print(\"len(model.layers):\", len(model.layers)) # 1\n",
        "##loss = ret.history['loss']\n",
        "##plt.plot(loss)\n",
        "##plt.xlabel('epochs')\n",
        "##plt.ylabel('loss')\n",
        "##plt.show()\n",
        "\n",
        "#3\n",
        "##print(model.get_weights())\n",
        "print(\"weights:\", model.layers[0].weights[0].numpy())\n",
        "print(\"bias:\", model.layers[0].bias.numpy())\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X, y_true, verbose=2)\n",
        "y_pred = model.predict(X)\n",
        "print(\"y_pred:\", y_pred)\n",
        "\n",
        "y_label = np.argmax(y_pred, axis = 1)\n",
        "print(\"y_label:\", y_label)\n",
        "\n",
        "#4: calculate the decision boundary\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "label = y_true.flatten()\n",
        "##plt.scatter(X[:, 0], X[:,1], c=label, s = 100)\n",
        "plt.scatter(X[label==0, 0], X[label==0,1], marker='x', s = 100)\n",
        "plt.scatter(X[label==1, 0], X[label==1,1], marker='o', s = 100)\n",
        "\n",
        "##for x,target in zip(X, y_true):\n",
        "##        plt.plot(x[0],x[1],'go' if (target == 1.0) else 'bx')\n",
        "\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min()-h, X[:, 0].max()+h\n",
        "y_min, y_max = X[:, 1].min()-h, X[:, 1].max()+h\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "sample = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model.predict(sample)\n",
        "Z = np.argmax(Z, axis = 1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contour(xx, yy, Z, colors='red', linewidths=2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h_mT4GZ9B32Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1902\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]], dtype = np.float32)\n",
        "\n",
        "y_true = np.array([[0],[0], [0],[1]], dtype = np.float32)   # AND\n",
        "##y_true = np.array([[0],[1],[1],[1]], dtype = np.float32)  # OR\n",
        "y_true = tf.keras.utils.to_categorical(y_true) \n",
        "print(\"y_true=\", y_true)\n",
        "\n",
        "#2\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=2, input_dim=2, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "##opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "ret = model.fit(X, y_true, epochs=100, batch_size=4, verbose=0)\n",
        "##print(\"len(model.layers):\", len(model.layers)) # 1\n",
        "##loss = ret.history['loss']\n",
        "##plt.plot(loss)\n",
        "##plt.xlabel('epochs')\n",
        "##plt.ylabel('loss')\n",
        "##plt.show()\n",
        "\n",
        "#3\n",
        "##print(model.get_weights())\n",
        "print(\"weights:\", model.layers[0].weights[0].numpy())\n",
        "print(\"bias:\", model.layers[0].bias.numpy())\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X, y_true, verbose=2)\n",
        "y_pred = model.predict(X)\n",
        "print(\"y_pred:\", y_pred)\n",
        "\n",
        "y_label = np.argmax(y_pred, axis = 1)\n",
        "print(\"y_label:\", y_label)\n",
        "\n",
        "#4 calculate the decision boundary\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "label = np.argmax(y_true, axis = 1)\n",
        "##plt.scatter(X[:, 0], X[:,1], c=label, s = 100)\n",
        "plt.scatter(X[label==0, 0], X[label==0,1], marker='x', s = 100)\n",
        "plt.scatter(X[label==1, 0], X[label==1,1], marker='o', s = 100)\n",
        "\n",
        "##for x,target in zip(X, y_true):\n",
        "##        plt.plot(x[0],x[1],'go' if (target == 1.0) else 'bx')\n",
        "\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min()-h, X[:, 0].max()+h\n",
        "y_min, y_max = X[:, 1].min()-h, X[:, 1].max()+h\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "sample = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model.predict(sample)\n",
        "Z = np.argmax(Z, axis = 1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contour(xx, yy, Z, colors='red', linewidths=2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "csStq0ixB8vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2001\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]], dtype = np.float32)\n",
        "\n",
        "##y_true = np.array([[0],[0], [0],[1]], dtype = np.float32)# AND\n",
        "##y_true = np.array([[0],[1],[1],[1]], dtype = np.float32) # OR\n",
        "y_true = np.array([[0],[1],[1],[0]], dtype = np.float32)   # XOR\n",
        "\n",
        "#2\n",
        "n = 2  # 10,  number of neurons in a hidden layer\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=n, input_dim=2, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "##model = tf.keras.Sequential()\n",
        "##model.add(tf.keras.layers.Input(shape = (2,))) # shape = 2\n",
        "##model.add(tf.keras.layers.Dense(units=n))\n",
        "##model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "##model.add(tf.keras.layers.Dense(units=1))\n",
        "##model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "##model.summary()\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "model.compile(optimizer=opt,loss='mse', metrics=['accuracy'])\n",
        "##model.compile(optimizer=opt,loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "ret = model.fit(X, y_true, epochs=2000, batch_size=4, verbose=0)\n",
        "##print(\"len(model.layers):\", len(model.layers)) # 2\n",
        "##loss = ret.history['loss']\n",
        "##plt.plot(loss)\n",
        "##plt.xlabel('epochs')\n",
        "##plt.ylabel('loss')\n",
        "##plt.show()\n",
        "\n",
        "#3\n",
        "##print(model.get_weights())\n",
        "##for i in range(len(model.layers)):\n",
        "##    print(\"layer :\", i, '-'*20)\n",
        "##    w = model.layers[i].weights[0].numpy()\n",
        "##    b = model.layers[i].bias.numpy()\n",
        "##    print(\"weights[{}]: {}\".format(i, np.array2string(w)))\n",
        "##    print(\"bias[{}]:    {}\".format(i, np.array2string(b)))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X, y_true, verbose=2)\n",
        "##y_pred = model.predict(X)\n",
        "##print(\"y_pred:\", y_pred)\n",
        "##\n",
        "##y_label = (y_pred> 0.5).astype(int)\n",
        "##print(\"y_label:\", y_label)\n",
        "\n",
        "#4: calculate the decision boundary\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "label = y_true.flatten()\n",
        "plt.scatter(X[label==0, 0], X[label==0,1], marker='x', s = 100)\n",
        "plt.scatter(X[label==1, 0], X[label==1,1], marker='o', s = 100)\n",
        "##for x,target in zip(X, y_true):\n",
        "##        plt.plot(x[0],x[1],'go' if (target == 1.0) else 'bx')\n",
        "\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min()-h, X[:, 0].max()+h\n",
        "y_min, y_max = X[:, 1].min()-h, X[:, 1].max()+h\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "sample = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model.predict(sample)\n",
        "Z = (Z> 0.5).astype(int) # Z = np.round(Z)\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contour(xx, yy, Z, colors='red', linewidths=2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oATtYUh0B9zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2002\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]], dtype = np.float32)\n",
        "\n",
        "# loss='sparse_categorical_crossentropy'\n",
        "##y_true = np.array([[0],[0], [0],[1]], dtype = np.float32)# AND\n",
        "##y_true = np.array([[0],[1],[1],[1]], dtype = np.float32) # OR\n",
        "y_true = np.array([[0],[1],[1],[0]], dtype = np.float32)   # XOR\n",
        "y_true = tf.keras.utils.to_categorical(y_true) # loss='categorical_crossentropy'\n",
        "print(\"y_true=\", y_true)\n",
        "\n",
        "#2\n",
        "model = tf.keras.Sequential()\n",
        "n = 2  # number of neurons in a hidden layer\n",
        "model.add(tf.keras.layers.Dense(units=n, input_dim=2, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(units=2, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "##model.compile(optimizer=opt,\n",
        "##              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "ret = model.fit(X, y_true, epochs=1000, batch_size=4, verbose=0)\n",
        "##print(\"len(model.layers):\", len(model.layers)) # 2\n",
        "##loss = ret.history['loss']\n",
        "##plt.plot(loss)\n",
        "##plt.xlabel('epochs')\n",
        "##plt.ylabel('loss')\n",
        "##plt.show()\n",
        "\n",
        "#3\n",
        "##print(model.get_weights())\n",
        "##for i in range(len(model.layers)):\n",
        "##    print(\"layer :\", i, '-'*20)\n",
        "##    w = model.layers[i].weights[0].numpy()\n",
        "##    b = model.layers[i].bias.numpy()\n",
        "##    print(\"weights[{}]: {}\".format(i, np.array2string(w)))\n",
        "##    print(\"bias[{}]:    {}\".format(i, np.array2string(b)))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X, y_true, verbose=2)\n",
        "##y_pred = model.predict(X)\n",
        "##print(\"y_pred:\", y_pred)\n",
        "##\n",
        "##y_label = np.argmax(y_pred, axis = 1)\n",
        "##print(\"y_label:\", y_label)\n",
        "\n",
        "#4: calculate the decision boundary\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "##label = y_true.flatten()            # loss='sparse_categorical_crossentropy'\n",
        "label = np.argmax(y_true, axis = 1) # loss='categorical_crossentropy'\n",
        "plt.scatter(X[label==0, 0], X[label==0,1], marker='x', s = 100)\n",
        "plt.scatter(X[label==1, 0], X[label==1,1], marker='o', s = 100)\n",
        "##for x,target in zip(X, y_true):\n",
        "##        plt.plot(x[0],x[1],'go' if (target == 1.0) else 'bx')\n",
        "\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min()-h, X[:, 0].max()+h\n",
        "y_min, y_max = X[:, 1].min()-h, X[:, 1].max()+h\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "sample = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model.predict(sample)\n",
        "Z = np.argmax(Z, axis = 1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contour(xx, yy, Z, colors='red', linewidths=2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pIC1wxUPCDXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2102\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "def createData(N=50):   \n",
        "    np.random.seed(1)\n",
        "    X0 = np.random.multivariate_normal(mean=[0.0, 0.0], cov= [[0.02, 0], [0, 0.01]], size=N)\n",
        "    y_true0 = np.zeros(shape = (N,))\n",
        "\n",
        "    X1 = np.random.multivariate_normal(mean=[0.0, 0.8], cov= [[0.01, 0], [0, 0.01]], size=N)\n",
        "    y_true1 = np.ones(shape = (N,))\n",
        "\n",
        "    X2 = np.random.multivariate_normal(mean=[0.3, 0.3], cov= [[0.01, 0], [0, 0.01]], size=N)\n",
        "    y_true2 = np.ones(shape = (N,))*2\n",
        "\n",
        "    X3 = np.random.multivariate_normal(mean=[0.8, 0.3], cov= [[0.01, 0], [0, 0.02]], size=N)\n",
        "    y_true3 = np.ones(shape = (N,))*3\n",
        "\n",
        "    X = np.vstack((X0, X1, X2, X3))\n",
        "    y_true = np.hstack((y_true0, y_true1, y_true2, y_true3))\n",
        "    return X, y_true\n",
        "\n",
        "X, y_true = createData()   \n",
        "y_true = tf.keras.utils.to_categorical(y_true) # 'mse', 'categorical_crossentropy'\n",
        "##print(\"y_true=\", y_true)\n",
        "\n",
        "#2\n",
        "n = 10  # number of neurons in a hidden layer\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=n, input_dim=2, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(units=4, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "##model = tf.keras.Sequential()\n",
        "##model.add(tf.keras.layers.Input(shape = (2,))) # shape = 2\n",
        "##model.add(tf.keras.layers.Dense(units=n))\n",
        "##model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "##model.add(tf.keras.layers.Dense(units=4))\n",
        "##model.add(tf.keras.layers.Activation('softmax'))\n",
        "##model.summary()\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n",
        "##model.compile(optimizer=opt,loss='mse', metrics=['accuracy'])\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "##model.compile(optimizer=opt,\n",
        "##              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "ret = model.fit(X, y_true, epochs=100, verbose=0) # batch_size=32\n",
        "##print(\"len(model.layers):\", len(model.layers))  # 2\n",
        "##loss = ret.history['loss']\n",
        "##plt.plot(loss)\n",
        "##plt.xlabel('epochs')\n",
        "##plt.ylabel('loss')\n",
        "##plt.show()\n",
        "\n",
        "#3\n",
        "##print(model.get_weights())\n",
        "##for i in range(len(model.layers)):\n",
        "##    print(\"layer :\", i, '-'*20)\n",
        "##    w = model.layers[i].weights[0].numpy()\n",
        "##    b = model.layers[i].bias.numpy()\n",
        "##    print(\"weights[{}]: {}\".format(i, np.array2string(w)))\n",
        "##    print(\"bias[{}]:    {}\".format(i, np.array2string(b)))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X, y_true, verbose=2)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "##print(\"y_pred:\", y_pred)\n",
        "y_label = np.argmax(y_pred, axis = 1)\n",
        "##print(\"y_label:\", y_label)\n",
        "\n",
        "C = tf.math.confusion_matrix(np.argmax(y_true, axis = 1), y_label)\n",
        "print(\"confusion_matrix(C):\", C)\n",
        "\n",
        "#4: calculate the decision boundary\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6, 6)\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "markers = \"ox+*\"\n",
        "colors  = \"bgcm\"\n",
        "labels  = (\"X0\", \"X1\", \"X2\", \"X3\")\n",
        "##label = y_true.flatten()          # loss='sparse_categorical_crossentropy'\n",
        "label = np.argmax(y_true, axis = 1) # loss='mse', 'categorical_crossentropy'\n",
        "for i, k in enumerate(np.unique(label)):\n",
        "    plt.scatter(X[label==k, 0], X[label==k, 1],\n",
        "                c = colors[i], marker=markers[i], label = labels[i])\n",
        "plt.legend()\n",
        "\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min()-h, X[:, 0].max()+h\n",
        "y_min, y_max = X[:, 1].min()-h, X[:, 1].max()+h\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "sample = np.c_[xx.ravel(), yy.ravel()]\n",
        "Z = model.predict(sample)\n",
        "Z = np.argmax(Z, axis = 1)\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contour(xx, yy, Z, colors='red', linewidths=2)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qx7viuGICE24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2201\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "'''\n",
        "ref1: https://en.wikipedia.org/wiki/Iris_flower_data_set#Data_set\n",
        "ref2: https://gist.github.com/curran/a08a1080b88344b0c8a7#file-iris-csv\n",
        "'''\n",
        "#1\n",
        "def load_Iris(shuffle=True):   \n",
        "    label={'setosa':0, 'versicolor':1, 'virginica':2}\n",
        "    data = np.loadtxt(\"./Data/iris.csv\", skiprows=1, delimiter=',',\n",
        "                      converters={4: lambda name: label[name.decode()]})\n",
        "    if shuffle:\n",
        "        np.random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "##iris_data = load_Iris(shuffle=True)    \n",
        "iris_data = load_Iris()\n",
        "X      = iris_data[:,:-1]\n",
        "y_true = iris_data[:, -1]\n",
        "    \n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"y_true.shape:\", y_true.shape)\n",
        "print(X[:3])\n",
        "print(y_true[:3])\n",
        "\n",
        "#2\n",
        "markers= \"ox+*sd\"\n",
        "colors = \"bgcmyk\"\n",
        "labels = [\"Iris setosa\",\"Iris versicolor\", \"Iris virginica\"]\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(6,6)\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Sepal Width')\n",
        "for i, k in enumerate(np.unique(y_true)):\n",
        "  plt.scatter(X[y_true== k, 0], # Sepal Length\n",
        "              X[y_true== k, 1], # Sepal Width\n",
        "              c=colors[i], marker=markers[i], label=labels[i])\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n",
        "#3\n",
        "plt.xlabel('Petal Length')\n",
        "plt.ylabel('Petal Width')\n",
        "for i, k in enumerate(np.unique(y_true)):\n",
        "  plt.scatter(X[y_true== k, 2], # Petal Length\n",
        "              X[y_true== k, 3], # Petal Width\n",
        "              c=colors[i], marker=markers[i], label=labels[i])\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iNWTW9j1CMpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2202\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#1\n",
        "def load_Iris(shuffle=False):   \n",
        "    label={'setosa':0, 'versicolor':1, 'virginica':2}\n",
        "    data = np.loadtxt(\"./Data/iris.csv\", skiprows=1, delimiter=',',\n",
        "                      converters={4: lambda name: label[name.decode()]})\n",
        "    if shuffle:\n",
        "        np.random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "def train_test_data_set(iris_data, test_rate=0.2): # train: 0.8, test: 0.2\n",
        "    n = int(iris_data.shape[0]*(1-test_rate))\n",
        "    x_train = iris_data[:n,:-1]\n",
        "    y_train = iris_data[:n, -1]\n",
        "    \n",
        "    x_test = iris_data[n:,:-1]\n",
        "    y_test = iris_data[n:,-1]\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "    \n",
        "iris_data = load_Iris(shuffle=True)\n",
        "(x_train, y_train), (x_test, y_test) = train_test_data_set(iris_data, test_rate=0.2)\n",
        "print(\"x_train.shape:\", x_train.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"x_test.shape:\",  x_test.shape)\n",
        "print(\"y_test.shape:\",  y_test.shape)\n",
        "  \n",
        "# one-hot encoding: 'mse', 'categorical_crossentropy'  \n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "##print(\"y_train=\", y_train)\n",
        "##print(\"y_test=\", y_test)\n",
        "\n",
        "#2\n",
        "n = 10  # number of neurons in a hidden layer\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units=n, input_dim=4, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(units=3, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "#3\n",
        "def MSE(y, t):\n",
        "    return tf.reduce_mean(tf.square(y - t)) # (y - t)**2\n",
        "\n",
        "CCE = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "##model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n",
        "##model.compile(optimizer=opt, loss= MSE, metrics=['accuracy'])\n",
        "##model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer=opt, loss= CCE, metrics=['accuracy'])\n",
        "\n",
        "ret = model.fit(x_train, y_train, epochs=100, verbose=0) # batch_size=32\n",
        "print(\"len(model.layers):\", len(model.layers))  # 2\n",
        "loss = ret.history['loss']\n",
        "plt.plot(loss)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "#4\n",
        "##print(model.get_weights())\n",
        "##for i in range(len(model.layers)):\n",
        "##    print(\"layer :\", i, '-'*20)\n",
        "##    w = model.layers[i].weights[0].numpy()\n",
        "##    b = model.layers[i].bias.numpy()\n",
        "##    print(\"weights[{}]: {}\".format(i, np.array2string(w)))\n",
        "##    print(\"bias[{}]:    {}\".format(i, np.array2string(b)))\n",
        "\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "y_pred = model.predict(x_train)\n",
        "y_label = np.argmax(y_pred, axis = 1)\n",
        "C = tf.math.confusion_matrix(np.argmax(y_train, axis = 1), y_label)\n",
        "print(\"confusion_matrix(C):\", C)\n"
      ],
      "metadata": {
        "id": "QAUV-g51CQ0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}