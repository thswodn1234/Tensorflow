{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 훈련 조기 종료(Early Stopping)"
      ],
      "metadata": {
        "id": "TNGz_Sl4adEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la_WxE9taMXR"
      },
      "outputs": [],
      "source": [
        "# MNIST 데이터 로드 후 정규화(Normalization)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#2:normalize images\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255.0 # [0, 1]\n",
        "x_test  /= 255.0\n",
        "\n",
        "# 레이블을 모두 원 핫 인코딩 벡터(One-hot encoding vector)로 변환\n",
        "\n",
        "#3: one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train) # (60000, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)   # (10000, 10)\n",
        "\n",
        "# 완전연결층(Dense layer) 사용해서 다층 퍼셉트론(MLP) 모델 구축하기\n",
        "\n",
        "#4: x_train.shape = (60000, 28, 28)\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(units=5, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "##model.summary()  이 코드를 사용하면 모델 구축 결과를 요약해서 볼 수 있다.\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# 모델의 loss는 크로스 엔트로피, 모델 성능 측정 지표는 Accuracy rate(정확도) 를 사용한다.\n",
        "\n",
        "# 콜백 설정: 특정 조건에서 모델 조기 종료 설정\n",
        "#5\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                            min_delta = 0.001,\n",
        "                                            patience=1,\n",
        "                                            verbose=2,\n",
        "                                            mode = 'auto') #'min','max', 'auto'  # mode에 min이 주어지면 모니터링 값 감소할 때 stopping\n",
        "                                                                                 # mode에 max가 주어지면 모니터링 값 감소할 때 stopping \n",
        "# #6\n",
        "# ret = model.fit(x_train, y_train, epochs=100, batch_size=200,\n",
        "#                 validation_split=0.2, verbose=2, callbacks=[callback])\n",
        "\n",
        "# # 모델 학습 스케쥴러 정의하기\n",
        "\n",
        "# pdf page 4\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch % 2 == 0 and epoch:\n",
        "        return 0.1*lr\n",
        "    return lr\n",
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n",
        "\n",
        "# 정의한 모델 학습 스케쥴러에 따라 모델 훈련시키기\n",
        "\n",
        "#6\n",
        "ret = model.fit(x_train, y_train, epochs=10, batch_size=200,\n",
        "                validation_split=0.2, verbose=0, callbacks=[callback])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텐서보드 사용하기"
      ],
      "metadata": {
        "id": "TSkLMdICpigJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "#1\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#2:normalize images\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255.0 # [0, 1]\n",
        "x_test  /= 255.0\n",
        "\n",
        "#3: one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train) # (60000, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)   # (10000, 10)\n",
        "\n",
        "#4: x_train.shape = (60000, 28, 28)\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(units=5, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "model.summary() \n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "EvTnB80tpkfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서보드 활성화 하고, 아래 코드 사용해 텐서보드 이용하기\n",
        "\n",
        "import os\n",
        "path = \"/content\"\n",
        "if not os.path.isdir(path):\n",
        "    os.mkdir(path)\n",
        "logdir = path + \"3101\"\n",
        "\n",
        "callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, update_freq ='epoch',\n",
        "                                          histogram_freq =10, write_images=True)\n",
        "#6\n",
        "ret = model.fit(x_train, y_train, epochs=10, batch_size=200,\n",
        "                validation_split=0.2, verbose=2, callbacks=[callback])\n",
        "\n"
      ],
      "metadata": {
        "id": "TyuOb0P0qcor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {logdir}"
      ],
      "metadata": {
        "id": "Gfo5fRxor5HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 08 그래디언트 소실과 가중치 초기화"
      ],
      "metadata": {
        "id": "auImHwTM1-U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST 데이터 로드후 He 초기화, ReLU 또는 LeakyReLU 함수 불러오기\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "#1\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#2:normalize images\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255.0 # [0, 1]\n",
        "x_test  /= 255.0\n",
        "\n",
        "#3: one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train) # (60000, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)   # (10000, 10)\n",
        "\n",
        "#4: build a model\n",
        "#4-1\n",
        "##init = tf.keras.initializers.he_normal() # 'he_nomal'\n",
        "##act = tf.keras.activations.relu          # 'relu'\n",
        "\n",
        "#4-2\n",
        "##init = tf.keras.initializers.he_normal() # 'he_normal'\n",
        "##act = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
        "\n",
        "#4-3\n",
        "init = tf.keras.initializers.he_uniform() # 'he_uniform'\n",
        "act = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
        "# init = tf.keras.initializers.RandomUniform(0.0, 1.0) \n",
        "# act = tf.keras.layers.sigmoid(alpha=0.3)\n",
        "# act = 'sigmoid'\n",
        "\n",
        "n = 100\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=10,activation='softmax', kernel_initializer=init))\n",
        "model.summary()\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#5: creates a summary file writer for the given log directory\n",
        "import os\n",
        "path = \"/content\"\n",
        "if not os.path.isdir(path):\n",
        "    os.mkdir(path)\n",
        "##logdir = path + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = path + \"3203\"\n",
        "\n",
        "file_writer = tf.summary.create_file_writer(logdir + \"/gradient\")\n",
        "file_writer.set_as_default()\n",
        "\n",
        "#6:  calculate averages and histograms of gradients in layers\n",
        "class GradientCallback(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, freq=10):\n",
        "##        super(GradientCallback, self).__init__()\n",
        "        self.freq = freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch%self.freq != 0:\n",
        "            return\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(x_train)  # tensor, logits\n",
        "            loss   = tf.keras.losses.binary_crossentropy(y_train, y_pred)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        for n in range(1, len(model.layers)):\n",
        "            i2 = (n-1)*2 # weights\n",
        "            i1 = i2 + 1  # biases\n",
        "\n",
        "            bias_avg   = tf.reduce_mean(tf.abs(grads[i1]))\n",
        "            weight_avg = tf.reduce_mean(tf.abs(grads[i2]))\n",
        "            \n",
        "            tf.summary.scalar(\"layer_%d/avg/bias\"%n, data=bias_avg, step=epoch)   \n",
        "            tf.summary.scalar(\"layer_%d/avg/weight\"%n, data=weight_avg, step=epoch)\n",
        "##            \n",
        "            tf.summary.histogram(\"layer_%d/hist/bias\"%n, data=grads[i1], step=epoch)\n",
        "            tf.summary.histogram(\"layer_%d/hist/weight\"%n, data=grads[i2], step=epoch)\n",
        "            \n",
        "    def on_train_end(self, logs):\n",
        "        tf.summary.flush()\n",
        "\n",
        "callback1 = GradientCallback() # freq = 10\n",
        "callback2 = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq= 10) #profile_batch=0      \n",
        "                                  \n",
        "#7: train and evaluate the model\n",
        "ret = model.fit(x_train, y_train, epochs=101, batch_size=200, validation_split=0.2,\n",
        "                 verbose=2, callbacks=[callback1, callback2])\n",
        "\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "metadata": {
        "id": "VtvHdPFv2EvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kill 34838\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {logdir}"
      ],
      "metadata": {
        "id": "cSptC-1-5Hfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 배치 정규화\n"
      ],
      "metadata": {
        "id": "D2KbbvfPXWDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "#1\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#2:normalize images\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255.0 # [0, 1]\n",
        "x_test  /= 255.0\n",
        "\n",
        "#3: one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train) # (60000, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)   # (10000, 10)\n",
        "\n",
        "#4: build a model\n",
        "init = tf.keras.initializers.RandomUniform(0.0, 1.0)\n",
        "##act = 'relu'\n",
        "act = tf.keras.layers.LeakyReLU(alpha = 0.3 )\n",
        "\n",
        "n = 100\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer=init))\n",
        "model.add(tf.keras.layers.Dense(units=10,activation='softmax', kernel_initializer=init))\n",
        "model.summary()\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "bdNVYYUyaw1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6:  calculate averages and histograms of gradients in layers\n",
        "class GradientCallback(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, freq=10):\n",
        "##        super(GradientCallback, self).__init__()\n",
        "        self.freq = freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch%self.freq != 0:\n",
        "            return\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(x_train)  # tensor, logits\n",
        "            loss   = tf.keras.losses.binary_crossentropy(y_train, y_pred)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        for n in range(1, len(model.layers)):\n",
        "            i2 = (n-1)*2 # weights\n",
        "            i1 = i2 + 1  # biases\n",
        "\n",
        "            bias_avg   = tf.reduce_mean(tf.abs(grads[i1]))\n",
        "            weight_avg = tf.reduce_mean(tf.abs(grads[i2]))\n",
        "            \n",
        "            tf.summary.scalar(\"layer_%d/avg/bias\"%n, data=bias_avg, step=epoch)   \n",
        "            tf.summary.scalar(\"layer_%d/avg/weight\"%n, data=weight_avg, step=epoch)\n",
        "##            \n",
        "            tf.summary.histogram(\"layer_%d/hist/bias\"%n, data=grads[i1], step=epoch)\n",
        "            tf.summary.histogram(\"layer_%d/hist/weight\"%n, data=grads[i2], step=epoch)\n",
        "            \n",
        "    def on_train_end(self, logs):\n",
        "        tf.summary.flush()\n",
        "\n",
        "callback1 = GradientCallback() # freq = 10\n",
        "callback2 = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq= 10) #profile_batch=0 \n",
        "\n",
        "#7: train and evaluate the model\n",
        "ret = model.fit(x_train, y_train, epochs=101, batch_size=200, validation_split=0.2,\n",
        "                 verbose=2, callbacks=[callback1, callback2])\n",
        "\n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "metadata": {
        "id": "YwfntdYNcQuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 과적합, 가중치 규제"
      ],
      "metadata": {
        "id": "IIW4_CQ6gQBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#1\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# subsampling for overfitting\n",
        "n_sample = 6000\n",
        "x_train = x_train[:n_sample]\n",
        "y_train = y_train[:n_sample]\n",
        "\n",
        "#2:normalize images\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255.0 # [0, 1]\n",
        "x_test  /= 255.0\n",
        "\n",
        "#3: one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train) # (n_sample, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)   # (10000,    10)\n",
        "\n",
        "#4: build a model without regularization\n",
        "act = \"relu\"\n",
        "init = \"he_uniform\"\n",
        "n = 100\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "#4-1: configure the model for training\n",
        "opt = 'rmsprop' # tf.keras.optimizers.RMSprop(learning_rate=0.001) \n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#4-2: train and evaluate the model\n",
        "ret = model.fit(x_train, y_train, epochs=101, batch_size=400,\n",
        "                                  validation_data = (x_test, y_test), verbose=0)\n",
        " \n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss, test_acc   = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "#4-3: plot accuracies\n",
        "plt.title(\"Without regularization by %s traing data in mnist\"%n_sample)\n",
        "plt.plot(ret.history['accuracy'],     \"b-\", label=\"train accuracy\")\n",
        "plt.plot(ret.history['val_accuracy'], \"r-\", label=\"val accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OyAxQIq3gSmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5: build a model with weight regularization\n",
        "reg = tf.keras.regularizers.l2(0.01)  # L2: 0.01, 0.1, 0.5\n",
        "model2 = tf.keras.Sequential()\n",
        "model2.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model2.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init,\n",
        "                                 kernel_regularizer=reg))\n",
        "model2.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init,\n",
        "                                 kernel_regularizer=reg))\n",
        "model2.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "##model2.summary()\n",
        " \n",
        "#5-1: configure the model for training\n",
        "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "                                 \n",
        "#5-2: train and evaluate the model\n",
        "ret2 = model2.fit(x_train, y_train, epochs=201, batch_size=400,\n",
        "                                    validation_data = (x_test, y_test), verbose=0)\n",
        "train_loss2, train_acc2 = model2.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss2,  test_acc2  = model2.evaluate(x_test,  y_test,  verbose=2)\n",
        "\n",
        "#5-3: plot accuracy\n",
        "plt.title(\"With regularization by %s traing data in mnist\"%n_sample)\n",
        "plt.plot(ret2.history['accuracy'],     \"b-\", label=\"train accuracy\")\n",
        "plt.plot(ret2.history['val_accuracy'], \"r-\", label=\"val accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E-62gRcZixAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 드롭아웃"
      ],
      "metadata": {
        "id": "xxDRrCB2tJ3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#1\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# subsampling for overfitting\n",
        "n_sample = 6000\n",
        "x_train = x_train[:n_sample]\n",
        "y_train = y_train[:n_sample]\n",
        "\n",
        "#2:normalize images\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255.0 # [0, 1]\n",
        "x_test  /= 255.0\n",
        "\n",
        "#3: one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train) # (n_sample, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)   # (10000,    10)\n",
        "\n",
        "#4: build a model without regularization\n",
        "act = \"relu\"\n",
        "init = \"he_uniform\"\n",
        "\n",
        "n = 100\n",
        "dropout_rate = 0.2\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dropout(rate = dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dropout(rate = dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 10, activation = 'softmax'))\n",
        "model.summary()\n",
        "\n",
        "#4-1: configure the model for training\n",
        "# opt = 'rmsprop' # tf.keras.optimizers.RMSprop(learning_rate=0.001) \n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#4-2: train and evaluate the model\n",
        "ret = model.fit(x_train, y_train, epochs=201, batch_size=400,\n",
        "                                  validation_data = (x_test, y_test), verbose=0)\n",
        " \n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss, test_acc   = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "#4-3: plot accuracies\n",
        "plt.title(\"Dropout rate = %s, %s traing data in mnist\" %(dropout_rate,n_sample))\n",
        "plt.plot(ret.history['accuracy'],     \"b-\", label=\"train accuracy\")\n",
        "plt.plot(ret.history['val_accuracy'], \"r-\", label=\"val accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Wud7QFri_T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.5\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dropout(rate = dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dropout(rate = dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 10, activation = 'softmax'))\n",
        "model.summary()\n",
        "\n",
        "#4-1: configure the model for training\n",
        "# opt = 'rmsprop' # tf.keras.optimizers.RMSprop(learning_rate=0.001) \n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#4-2: train and evaluate the model\n",
        "ret = model.fit(x_train, y_train, epochs=201, batch_size=400,\n",
        "                                  validation_data = (x_test, y_test), verbose=0)\n",
        " \n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss, test_acc   = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "#4-3: plot accuracies\n",
        "plt.title(\"Dropout rate = %s, %s traing data in mnist\" %(dropout_rate,n_sample))\n",
        "plt.plot(ret.history['accuracy'],     \"b-\", label=\"train accuracy\")\n",
        "plt.plot(ret.history['val_accuracy'], \"r-\", label=\"val accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QhdfjiPlvOD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 드롭아웃: CIFAR-10\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#1\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#2:normalize images\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255.0 # [0, 1]\n",
        "x_test  /= 255.0\n",
        "\n",
        "#3: one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train) # (50000, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)   # (10000,    10)\n",
        "\n",
        "#4: build a model with dropout\n",
        "act = tf.keras.layers.LeakyReLU(alpha=0.3)\n",
        "init = \"he_uniform\"\n",
        "n = 100\n",
        "dropout_rate = 0.2\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape = (32,32,3)))\n",
        "model.add(tf.keras.layers.Dense(units = n, activation = act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dropout(rate = dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = n, activation = act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dropout(rate = dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 10, activation = \"softmax\"))\n",
        "\n",
        "\n",
        "#4-1: configure the model for training\n",
        "# opt = 'rmsprop' # tf.keras.optimizers.RMSprop(learning_rate=0.001) \n",
        "model.compile(optimizer='opt', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#4-2: train and evaluate the model\n",
        "ret = model.fit(x_train, y_train, epochs=201, batch_size=400,\n",
        "                                  validation_data = (x_test, y_test), verbose=2)\n",
        " \n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss, test_acc   = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "#4-3: plot accuracies\n",
        "plt.plot(ret.history['accuracy'],     \"b-\", label=\"train accuracy\")\n",
        "plt.plot(ret.history['val_accuracy'], \"r-\", label=\"val accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "brz4ng1hzKPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#1\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#2:normalize images\n",
        "x_train = x_train.astype('float32')\n",
        "x_test  = x_test.astype('float32')\n",
        "x_train /= 255.0 # [0, 1]\n",
        "x_test  /= 255.0\n",
        "\n",
        "#3: one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train) # (50000, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)   # (10000, 10)\n",
        "\n",
        "#4: build a model with dropout\n",
        "act =  tf.keras.layers.LeakyReLU(alpha=0.3) #'relu','sigmoid'\n",
        "init = 'he_uniform'\n",
        "n = 100\n",
        "dropout_rate = 0.2 # 0.5\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dropout( rate=dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=n, activation=act, kernel_initializer = init))\n",
        "model.add(tf.keras.layers.Dropout( rate=dropout_rate))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "#4-1: configure the model for training\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#4-2: train and evaluate the model\n",
        "ret = model.fit(x_train, y_train, epochs=201, batch_size=400,\n",
        "                                  validation_data = (x_test, y_test), verbose=0)\n",
        " \n",
        "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
        "test_loss, test_acc   = model.evaluate(x_test,  y_test, verbose=2)\n",
        "\n",
        "#4-3: plot accuracies\n",
        "plt.plot(ret.history['accuracy'],     \"b-\", label=\"train accuracy\")\n",
        "plt.plot(ret.history['val_accuracy'], \"r-\", label=\"val accuracy\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eOrAtrRT3TjI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}