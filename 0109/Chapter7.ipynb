{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCq2UCUvEkhh"
      },
      "source": [
        "<시작하세요! 텐서플로 2.0 프로그래밍> 예제코드입니다.\n",
        "\n",
        "- 예제 코드를 실행하기 위해서는 [파일] > [드라이브에 사본 저장]으로 본인의 계정에 사본을 만든 다음 실행하면 됩니다.\n",
        "- 예제 코드는 [깃허브 저장소](https://github.com/wikibook/tf2)에서도 동일하게 제공됩니다. 예제에 대한 질문이나 책에 대한 질문은 깃허브 저장소의 [Issues](https://github.com/wikibook/tf2/issues)에 올려주세요.\n",
        "- 각 장의 예제 파일은 처음부터 끝까지 실행하는 상황을 가정하고 작성되었습니다. 혹시 세션이 다운되는 등의 이유로 실행이 되지 않는 경우가 있다면, 필요한 라이브러리를 import 하신 후에 실행하시면 됩니다. (tensorflow, numpy, pandas 등)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl58Zr-ZKjRE"
      },
      "source": [
        "# 텐서플로 2 버전 선택\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSciZqogK0Mg"
      },
      "source": [
        "# 7.2 주요 레이어 정리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DmYqaWfcxJa"
      },
      "source": [
        "## 7.2.1 SimpleRNN 레이어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0f2FG66cx27"
      },
      "source": [
        "# 7.1 SimpleRNN 레이어 생성 코드\n",
        "rnn1 = tf.keras.layers.SimpleRNN(units=1, activation='tanh', return_sequences=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU4_rMdw47Au"
      },
      "source": [
        "# 7.2 시퀀스 예측 데이터 생성\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(6):\n",
        "    # [0,1,2,3], [1,2,3,4] 같은 정수의 시퀀스를 만듭니다.\n",
        "    lst = list(range(i,i+4))\n",
        "\n",
        "    # 위에서 구한 시퀀스의 숫자들을 각각 10으로 나눈 다음 저장합니다.\n",
        "    # SimpleRNN 에 각 타임스텝에 하나씩 숫자가 들어가기 때문에 여기서도 하나씩 분리해서 배열에 저장합니다.\n",
        "    X.append(list(map(lambda c: [c/10], lst)))\n",
        "\n",
        "    # 정답에 해당하는 4, 5 등의 정수를 역시 위처럼 10으로 나눠서 저장합니다.\n",
        "    Y.append((i+4)/10)\n",
        "    \n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "for i in range(len(X)):\n",
        "    print(X[i], Y[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFdjOc6G5kO1"
      },
      "source": [
        "# 7.3 시퀀스 예측 모델 정의\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(units=10, return_sequences=False, input_shape=[4,1]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LUly58bAGm3D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7odqgVWw5oyH"
      },
      "source": [
        "# 7.4 네트워크 훈련 및 결과 확인\n",
        "model.fit(X, Y, epochs=100, verbose=0)\n",
        "print(np.shape(X))\n",
        "print(model.predict(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQUaYRRG7aWA"
      },
      "source": [
        "# 7.5 학습되지 않은 시퀀스에 대한 예측 결과\n",
        "print(model.predict(np.array([[[0.6],[0.7],[0.8],[0.9]]])))\n",
        "print(model.predict(np.array([[[-0.1],[0.0],[0.1],[0.2]]])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGzdLJHZvSUg"
      },
      "source": [
        "## 7.2.2 LSTM 레이어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYyU1BKZFDvo"
      },
      "source": [
        "# 7.6 곱셈 문제 데이터 생성\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(3000):\n",
        "    # 0~1 사이의 랜덤한 숫자 100 개를 만듭니다.\n",
        "    lst = np.random.rand(100)\n",
        "    # 마킹할 숫자 2개의 인덱스를 뽑습니다.\n",
        "    idx = np.random.choice(100, 2, replace=False)\n",
        "    # 마킹 인덱스가 저장된 원-핫 인코딩 벡터를 만듭니다.\n",
        "    zeros = np.zeros(100)\n",
        "    zeros[idx] = 1\n",
        "    # 마킹 인덱스와 랜덤한 숫자를 합쳐서 X 에 저장합니다.\n",
        "    X.append(np.array(list(zip(zeros, lst))))\n",
        "    # 마킹 인덱스가 1인 값들만 서로 곱해서 Y 에 저장합니다.\n",
        "    Y.append(np.prod(lst[idx]))\n",
        "    \n",
        "print(X[0], Y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doitxA8uOm5i"
      },
      "source": [
        "# 7.7 SimpleRNN 레이어를 사용한 곱셈 문제 모델 정의\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.SimpleRNN(units=30, return_sequences=True, input_shape=[100,2]),\n",
        "    tf.keras.layers.SimpleRNN(units=30),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTp4nvHYOqOZ"
      },
      "source": [
        "# 7.8 SimpleRNN 네트워크 학습\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "# 2560개의 데이터만 학습시킵니다. validation 데이터는 20% 로 지정합니다.\n",
        "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EiSvd5rOsIV"
      },
      "source": [
        "# 7.9 SimpleRNN 네트워크 학습 결과 확인\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5OnirteOyut"
      },
      "source": [
        "# 7.10 Test 데이터에 대한 예측 정확도 확인\n",
        "model.evaluate(X[2560:], Y[2560:])\n",
        "prediction = model.predict(X[2560:2560+5])\n",
        "# 5개 테스트 데이터에 대한 예측을 표시합니다.\n",
        "for i in range(5):\n",
        "    print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
        "    \n",
        "prediction = model.predict(X[2560:])\n",
        "fail = 0\n",
        "for i in range(len(prediction)):\n",
        "    # 오차가 0.04 이상이면 오답입니다.\n",
        "    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
        "        fail += 1\n",
        "print('correctness:', (440 - fail) / 440 * 100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o1z24gWIe4B"
      },
      "source": [
        "# 7.11 LSTM 레이어를 사용한 곱셈 문제 모델 정의\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(units=30, return_sequences=True, input_shape=[100,2]),\n",
        "    tf.keras.layers.LSTM(units=30),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XavhYhX4Imax"
      },
      "source": [
        "# 7.12 LSTM 네트워크 학습\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJSlA-z7NRHE"
      },
      "source": [
        "# 7.13 LSTM 네트워크 학습 결과 확인\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob1VbaYzJCWa"
      },
      "source": [
        "# 7.14 Test 데이터에 대한 예측 정확도 확인\n",
        "model.evaluate(X[2560:], Y[2560:])\n",
        "prediction = model.predict(X[2560:2560+5])\n",
        "for i in range(5):\n",
        "    print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
        "    \n",
        "prediction = model.predict(X[2560:])\n",
        "cnt = 0\n",
        "for i in range(len(prediction)):\n",
        "    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
        "        cnt += 1\n",
        "print('correctness:', (440 - cnt) / 440 * 100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxV7T14AvXkH"
      },
      "source": [
        "## 7.2.3 GRU 레이어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WjH-A0WvNAO"
      },
      "source": [
        "# 7.15 GRU 레이어를 사용한 곱셈 문제 모델 정의\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.GRU(units=30, return_sequences=True, input_shape=[100,2]),\n",
        "    tf.keras.layers.GRU(units=30),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd6C4gvUvdQS"
      },
      "source": [
        "# 7.16 GRU 네트워크 학습\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "history = model.fit(X[:2560], Y[:2560], epochs=100, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SHz_LHExkH3"
      },
      "source": [
        "# 7.17 GRU 네트워크 학습 결과 확인\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdecdSb0xkDj"
      },
      "source": [
        "# 7.18 Test 데이터에 대한 예측 정확도 확인\n",
        "model.evaluate(X[2560:], Y[2560:])\n",
        "prediction = model.predict(X[2560:2560+5])\n",
        "for i in range(5):\n",
        "    print(Y[2560+i], '\\t', prediction[i][0], '\\tdiff:', abs(prediction[i][0] - Y[2560+i]))\n",
        "    \n",
        "prediction = model.predict(X[2560:])\n",
        "cnt = 0\n",
        "for i in range(len(prediction)):\n",
        "    if abs(prediction[i][0] - Y[2560+i]) > 0.04:\n",
        "        cnt += 1\n",
        "print('correctness:', (440 - cnt) / 440 * 100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4rFl018T7Py"
      },
      "source": [
        "# 7.3 긍정, 부정 감성 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srjpifEqUAx3"
      },
      "source": [
        "# 7.19 Naver Sentiment Movie Corpus v1.0 다운로드\n",
        "path_to_train_file = tf.keras.utils.get_file('train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
        "path_to_test_file = tf.keras.utils.get_file('test.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e3_cWrjcZ8X"
      },
      "source": [
        "# 7.20 데이터 로드 및 확인\n",
        "# 데이터를 메모리에 불러옵니다. encoding 형식으로 utf-8 을 지정해야합니다.\n",
        "train_text = open(path_to_train_file, 'rb').read().decode(encoding='utf-8')\n",
        "test_text = open(path_to_test_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 텍스트가 총 몇 자인지 확인합니다.\n",
        "print('Length of text: {} characters'.format(len(train_text)))\n",
        "print('Length of text: {} characters'.format(len(test_text)))\n",
        "print()\n",
        "\n",
        "# 처음 300 자를 확인해봅니다.\n",
        "print(train_text[:300])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R4G9NAekYsE"
      },
      "source": [
        "# 7.21 학습을 위한 정답 데이터(Y) 만들기\n",
        "train_Y = np.array([[int(row.split('\\t')[2])] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0])\n",
        "test_Y = np.array([[int(row.split('\\t')[2])] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0])\n",
        "print(train_Y.shape, test_Y.shape)\n",
        "print(train_Y[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlrwaPyJ06V5"
      },
      "source": [
        "# 7.22 train 데이터의 입력(X)에 대한 정제(Cleaning)\n",
        "import re\n",
        "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "def clean_str(string):    \n",
        "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "\n",
        "    return string.lower()\n",
        "\n",
        "\n",
        "train_text_X = [row.split('\\t')[1] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0]\n",
        "train_text_X = [clean_str(sentence) for sentence in train_text_X]\n",
        "# 문장을 띄어쓰기 단위로 단어 분리\n",
        "sentences = [sentence.split(' ') for sentence in train_text_X]\n",
        "for i in range(5):\n",
        "    print(sentences[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwtf77CE0xPi"
      },
      "source": [
        "# 7.23 각 문장의 단어 길이 확인\n",
        "import matplotlib.pyplot as plt\n",
        "sentence_len = [len(sentence) for sentence in sentences]\n",
        "sentence_len.sort()\n",
        "plt.plot(sentence_len)\n",
        "plt.show()\n",
        "\n",
        "print(sum([int(l<=25) for l in sentence_len]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9F0OQSB71Az"
      },
      "source": [
        "# 7.24 단어 정제 및 문장 길이 줄임\n",
        "sentences_new = []\n",
        "for sentence in sentences:\n",
        "    sentences_new.append([word[:5] for word in sentence][:25])\n",
        "sentences = sentences_new\n",
        "for i in range(5):\n",
        "    print(sentences[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iHRApH_SxJz"
      },
      "source": [
        "# 7.25 Tokenizer와 pad_sequences를 사용한 문장 전처리\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "train_X = tokenizer.texts_to_sequences(sentences)\n",
        "train_X = pad_sequences(train_X, padding='post')\n",
        "\n",
        "print(train_X[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id3SzN7350CB"
      },
      "source": [
        "# 7.26 Tokenizer의 동작 확인\n",
        "print(tokenizer.index_word[19999])\n",
        "print(tokenizer.index_word[20000])\n",
        "temp = tokenizer.texts_to_sequences(['#$#$#', '경우는', '잊혀질', '연기가'])\n",
        "print(temp)\n",
        "temp = pad_sequences(temp, padding='post')\n",
        "print(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHkCAL4QpVAF"
      },
      "source": [
        "# 7.27 감성 분석을 위한 모델 정의\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(20000, 300, input_length=25),\n",
        "    tf.keras.layers.LSTM(units=50),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG5bZtehpU8O"
      },
      "source": [
        "# 7.28 감성 분석 모델 학습\n",
        "history = model.fit(train_X, train_Y, epochs=5, batch_size=128, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N83MOKFpU4a"
      },
      "source": [
        "# 7.29 감성 분석 모델 학습 결과 확인\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], 'b-', label='loss')\n",
        "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], 'g-', label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], 'k--', label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylim(0.7, 1)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS4Bw55e1HQa"
      },
      "source": [
        "# 7.30 테스트 데이터 평가\n",
        "test_text_X = [row.split('\\t')[1] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0]\n",
        "test_text_X = [clean_str(sentence) for sentence in test_text_X]\n",
        "sentences = [sentence.split(' ') for sentence in test_text_X]\n",
        "sentences_new = []\n",
        "for sentence in sentences:\n",
        "    sentences_new.append([word[:5] for word in sentence][:25])\n",
        "sentences = sentences_new\n",
        "\n",
        "test_X = tokenizer.texts_to_sequences(sentences)\n",
        "test_X = pad_sequences(test_X, padding='post')\n",
        "\n",
        "model.evaluate(test_X, test_Y, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_gWAXeb1HM0"
      },
      "source": [
        "# 7.31 임의의 문장 감성 분석 결과 확인\n",
        "test_sentence = '재미있을 줄 알았는데 완전 실망했다. 너무 졸리고 돈이 아까웠다.'\n",
        "test_sentence = test_sentence.split(' ')\n",
        "test_sentences = []\n",
        "now_sentence = []\n",
        "for word in test_sentence:\n",
        "    now_sentence.append(word)\n",
        "    test_sentences.append(now_sentence[:])\n",
        "    \n",
        "test_X_1 = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_X_1 = pad_sequences(test_X_1, padding='post', maxlen=25)\n",
        "prediction = model.predict(test_X_1)\n",
        "for idx, sentence in enumerate(test_sentences):\n",
        "    print(sentence)\n",
        "    print(prediction[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbJuXY19T-vd"
      },
      "source": [
        "# 7.4 자연어 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3eau9dNxOgR"
      },
      "source": [
        "## 7.4.1 단어 단위 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWbhqXUTUBJj"
      },
      "source": [
        "# 7.32 조선왕조실록 데이터 파일 다운로드\n",
        "path_to_file = tf.keras.utils.get_file('input.txt', 'http://bit.ly/2Mc3SOV')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uFbN0cwulLE"
      },
      "source": [
        "# 7.33 데이터 로드 및 확인\n",
        "# 데이터를 메모리에 불러옵니다. encoding 형식으로 utf-8 을 지정해야합니다.\n",
        "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# 텍스트가 총 몇 자인지 확인합니다.\n",
        "print('Length of text: {} characters'.format(len(train_text)))\n",
        "print()\n",
        "\n",
        "# 처음 100 자를 확인해봅니다.\n",
        "print(train_text[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYpvgEtfulIe"
      },
      "source": [
        "# 7.34 훈련 데이터 입력 정제\n",
        "import re\n",
        "# From https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "def clean_str(string):    \n",
        "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \"\", string)\n",
        "    string = re.sub(r\"\\)\", \"\", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "\n",
        "    return string\n",
        "\n",
        "\n",
        "train_text = train_text.split('\\n')\n",
        "train_text = [clean_str(sentence) for sentence in train_text]\n",
        "train_text_X = []\n",
        "for sentence in train_text:\n",
        "    train_text_X.extend(sentence.split(' '))\n",
        "    train_text_X.append('\\n')\n",
        "    \n",
        "train_text_X = [word for word in train_text_X if word != '']\n",
        "\n",
        "print(train_text_X[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWMq4C-qulCn"
      },
      "source": [
        "# 7.35 단어 토큰화\n",
        "# 단어의 set을 만듭니다.\n",
        "vocab = sorted(set(train_text_X))\n",
        "vocab.append('UNK')\n",
        "print ('{} unique words'.format(len(vocab)))\n",
        "\n",
        "# vocab list를 숫자로 맵핑하고, 반대도 실행합니다.\n",
        "word2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2word = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([word2idx[c] for c in train_text_X])\n",
        "\n",
        "# word2idx 의 일부를 알아보기 쉽게 print 해봅니다.\n",
        "print('{')\n",
        "for word,_ in zip(word2idx, range(10)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
        "print('  ...\\n}')\n",
        "\n",
        "print('index of UNK: {}'.format(word2idx['UNK']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIOkvMpd_7Wd"
      },
      "source": [
        "# 7.36 토큰 데이터 확인\n",
        "print(train_text_X[:20])\n",
        "print(text_as_int[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKeZRJsM_7Tx"
      },
      "source": [
        "# 7.37 기본 데이터셋 만들기\n",
        "seq_length = 25\n",
        "examples_per_epoch = len(text_as_int) // seq_length\n",
        "sentence_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sentence_dataset = sentence_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sentence_dataset.take(1):\n",
        "    print(idx2word[item.numpy()])\n",
        "    print(item.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCgSGJTQ_7QH"
      },
      "source": [
        "# 7.38 학습 데이터셋 만들기\n",
        "def split_input_target(chunk):\n",
        "    return [chunk[:-1], chunk[-1]]\n",
        "\n",
        "train_dataset = sentence_dataset.map(split_input_target)\n",
        "for x,y in train_dataset.take(1):\n",
        "    print(idx2word[x.numpy()])\n",
        "    print(x.numpy())\n",
        "    print(idx2word[y.numpy()])\n",
        "    print(y.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6LSzyUgS72O"
      },
      "source": [
        "# 7.39 데이터셋 shuffle, batch 설정\n",
        "BATCH_SIZE = 512\n",
        "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGDQMKzZRz9k"
      },
      "source": [
        "# 7.40 단어 단위 생성 모델 정의\n",
        "total_words = len(vocab)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 100, input_length=seq_length),\n",
        "    tf.keras.layers.LSTM(units=100, return_sequences=True),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.LSTM(units=100),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)"
      ],
      "metadata": {
        "id": "yVnY2RB3mdTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCF1_hcARz6g"
      },
      "source": [
        "# 7.41 단어 단위 생성 모델 학습\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodel(epoch, logs):\n",
        "    if epoch % 5 != 0 and epoch != 49:\n",
        "        return\n",
        "    test_sentence = train_text[0]\n",
        "\n",
        "    next_words = 100\n",
        "    for _ in range(next_words):\n",
        "        test_text_X = test_sentence.split(' ')[-seq_length:]\n",
        "        test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
        "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "\n",
        "        output_idx = model.predict_classes(test_text_X)\n",
        "        test_sentence += ' ' + idx2word[output_idx[0]]\n",
        "    \n",
        "    print()\n",
        "    print(test_sentence)\n",
        "    print()\n",
        "\n",
        "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
        "\n",
        "history = model.fit(train_dataset.repeat(), epochs=50, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz6UCaidRz2b"
      },
      "source": [
        "# 7.42 임의의 문장을 사용한 생성 결과 확인\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "test_sentence = '동헌에 나가 공무를 본 후 활 십오 순을 쏘았다'\n",
        "\n",
        "next_words = 100\n",
        "for _ in range(next_words):\n",
        "    test_text_X = test_sentence.split(' ')[-seq_length:]\n",
        "    test_text_X = np.array([word2idx[c] if c in word2idx else word2idx['UNK'] for c in test_text_X])\n",
        "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=word2idx['UNK'])\n",
        "    \n",
        "    output_idx = model.predict_classes(test_text_X)\n",
        "    test_sentence += ' ' + idx2word[output_idx[0]]\n",
        "\n",
        "print(test_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hzr8inp0Go4"
      },
      "source": [
        "## 7.4.2 자소 단위 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPw8MxlxvE66"
      },
      "source": [
        "# 7.43 jamotools 설치\n",
        "!pip install jamotools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq6XfbRpvEz5"
      },
      "source": [
        "# 7.44 자모 분리 테스트\n",
        "import jamotools\n",
        "\n",
        "train_text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "s = train_text[:100]\n",
        "print(s)\n",
        "\n",
        "# 한글 텍스트를 자모 단위로 분리해줍니다. 한자 등에는 영향이 없습니다.\n",
        "s_split = jamotools.split_syllables(s)\n",
        "print(s_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ivfd0G5RRLJ"
      },
      "source": [
        "# 7.45 자모 결합 테스트\n",
        "s2 = jamotools.join_jamos(s_split)\n",
        "print(s2)\n",
        "print(s == s2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtl5EkcrRRIV"
      },
      "source": [
        "# 7.46 자모 토큰화\n",
        "# 텍스트를 자모 단위로 나눕니다. 데이터가 크기 때문에 약간 시간이 걸립니다.\n",
        "train_text_X = jamotools.split_syllables(train_text)\n",
        "vocab = sorted(set(train_text_X))\n",
        "vocab.append('UNK')\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# vocab list를 숫자로 맵핑하고, 반대도 실행합니다.\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in train_text_X])\n",
        "\n",
        "# word2idx 의 일부를 알아보기 쉽게 print 해봅니다.\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(10)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n",
        "\n",
        "print('index of UNK: {}'.format(char2idx['UNK']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char2idx)\n",
        "print(type(char2idx))"
      ],
      "metadata": {
        "id": "ugytBrxnpslA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opyAlE-ZR4-o"
      },
      "source": [
        "# 7.47 토큰 데이터 확인\n",
        "print(train_text_X[:20])\n",
        "print(text_as_int[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP0hfUCVRsp7"
      },
      "source": [
        "# 7.48 학습 데이터세트 생성\n",
        "seq_length = 80\n",
        "examples_per_epoch = len(text_as_int) // seq_length\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "char_dataset = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in char_dataset.take(1):\n",
        "    print(idx2char[item.numpy()])\n",
        "    print(item.numpy())\n",
        "    \n",
        "def split_input_target(chunk):\n",
        "    return [chunk[:-1], chunk[-1]]\n",
        "\n",
        "train_dataset = char_dataset.map(split_input_target)\n",
        "for x,y in train_dataset.take(1):\n",
        "    print(idx2char[x.numpy()])\n",
        "    print(x.numpy())\n",
        "    print(idx2char[y.numpy()])\n",
        "    print(y.numpy())\n",
        "    \n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TazYgfrsRsly"
      },
      "source": [
        "# 7.49 자소 단위 생성 모델 정의\n",
        "total_chars = len(vocab)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_chars, 100, input_length=seq_length),\n",
        "    tf.keras.layers.LSTM(units=400),\n",
        "    tf.keras.layers.Dense(total_chars, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjv4zsxFRRF9"
      },
      "source": [
        "# 7.50 자소 단위 생성 모델 학습\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def testmodel(epoch, logs):\n",
        "    if epoch % 5 != 0 and epoch != 99:\n",
        "        return\n",
        "    \n",
        "    test_sentence = train_text[:48]\n",
        "    test_sentence = jamotools.split_syllables(test_sentence)\n",
        "\n",
        "    next_chars = 300\n",
        "    for _ in range(next_chars):\n",
        "        test_text_X = test_sentence[-seq_length:]\n",
        "        test_text_X = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_X])\n",
        "        test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n",
        "\n",
        "        output_idx = model.predict_classes(test_text_X)\n",
        "        test_sentence += idx2char[output_idx[0]]\n",
        "    \n",
        "    print()\n",
        "    print(jamotools.join_jamos(test_sentence))\n",
        "    print()\n",
        "\n",
        "testmodelcb = tf.keras.callbacks.LambdaCallback(on_epoch_end=testmodel)\n",
        "\n",
        "history = model.fit(train_dataset.repeat(), epochs=100, steps_per_epoch=steps_per_epoch, callbacks=[testmodelcb], verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2PIJeXRvEw-"
      },
      "source": [
        "# 7.51 임의의 문장을 사용한 생성 결과 확인\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "test_sentence = '동헌에 나가 공무를 본 후 활 십오 순을 쏘았다'\n",
        "test_sentence = jamotools.split_syllables(test_sentence)\n",
        "\n",
        "next_chars = 300\n",
        "for _ in range(next_chars):\n",
        "    test_text_X = test_sentence[-seq_length:]\n",
        "    test_text_X = np.array([char2idx[c] if c in char2idx else char2idx['UNK'] for c in test_text_X])\n",
        "    test_text_X = pad_sequences([test_text_X], maxlen=seq_length, padding='pre', value=char2idx['UNK'])\n",
        "    \n",
        "    output_idx = model.predict_classes(test_text_X)\n",
        "    test_sentence += idx2char[output_idx[0]]\n",
        "    \n",
        "\n",
        "print(jamotools.join_jamos(test_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}